{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Strategy Learner\n",
    "ML for trading Udacity Course exercise\n",
    "\n",
    "More info:\n",
    "https://quantsoftware.gatech.edu/Strategy_learner\n",
    "\n",
    "A transcription of the Udacity Course lectures can be find on https://docs.google.com/document/d/1ELqlnuTSdc9-MDHOkV0uvSY4RmI1eslyQlU9DgOY_jc/edit?usp=sharing\n",
    "\n",
    "Kairoart 2018\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this project you will design a learning trading agent. You must draw on the learners you have created so far in the course. Your choices are:\n",
    "\n",
    "1. Regression or classification-based learner: Create a strategy using your Random Forest learner. Suggestions if you follow this approach: Classification_Trader_Hints. Important note, if you choose this method, you must set the leaf_size for your learner to 5 or greater. This is to avoid degenerate overfitting in-sample.\n",
    "2. Reinforcement Learner-based approach: Create a Q-learning-based strategy using your Q-Learner. Read the Classification_Trader_Hints first, because many of the ideas there are relevant for the Q trader, then see Q_Trader_Hints\n",
    "3. Optimization-based learner: Create a scan-based strategy using an optimizer. Read the Classification_Trader_Hints first, because many of the ideas there are relevant for the Opto trader, then see Opto_Trader_Hints\n",
    "\n",
    "Regardless of your choice above, your learner should work in the following way:\n",
    "\n",
    "* In the training phase (e.g., addEvidence()) your learner will be provided with a stock symbol and a time period. It should use this data to learn a strategy. For instance, for a regression-based learner it will use this data to make predictions about future price changes.\n",
    "* In the testing phase (e.g., testPolicy()) your learner will be provided a symbol and a date range. All learning should be turned OFF during this phase.\n",
    "\n",
    "\n",
    "If the date range is the same as used for the training, it is an in-sample test. Otherwise it is an out-of-sample test. Your learner should return a trades dataframe like it did in the last project. Here are some important requirements: Your testPolicy() method should be much faster than your addEvidence() method. The timeout requirements (see rubric) will be set accordingly. Multiple calls to your testPolicy() method should return exactly the same result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Devise numerical/technical indicators to evaluate the state of a stock on each day.\n",
    "* Build a strategy learner based on one of the learners described above that uses the indicators.\n",
    "* Test/debug the strategy learner on specific symbol/time period problems.\n",
    "* Write a report describing your learning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Details, Dates and Rules\n",
    "\n",
    "* For your report, trade only the symbol JPM. This will enable us to more easily compare results. We will test your learner with other symbols as well.\n",
    "* You may use data from other symbols (such as SPY) to inform your strategy.\n",
    "* The in sample/development period is January 1, 2008 to December 31 2009.\n",
    "* The out of sample/testing period is January 1, 2010 to December 31 2011.\n",
    "* Starting cash is 100,000.\n",
    "* Allowable positions are: 1000 shares long, 1000 shares short, 0 shares.\n",
    "* Benchmark: The performance of a portfolio starting with 100,000 cash, investing in 1000 shares of the symbol in use and holding that position. Include transaction costs.\n",
    "* There is no limit on leverage.\n",
    "* Transaction costs: Commission will always be 0.00, Impact may vary, and will be passed in as a parameter to the learner.\n",
    "* Minimize use of herrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Strategy Learner\n",
    "\n",
    "For this part of the project you should develop a learner that can learn a trading policy using your learner. You should be able to use your Q-Learner or RTLearner from the earlier project directly, with no changes. If you want to use the optimization approach, you will need to create new code or that. You will need to write code in StrategyLearner.py to \"wrap\" your learner appropriately to frame the trading problem for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StrategyLearner API\n",
    "\n",
    "import StrategyLearner as sl\n",
    "learner = sl.StrategyLearner(verbose = False, impact = 0.000) # constructor\n",
    "learner.addEvidence(symbol = \"AAPL\", sd=dt.datetime(2008,1,1), ed=dt.datetime(2009,12,31), sv = 100000) # training phase\n",
    "df_trades = learner.testPolicy(symbol = \"AAPL\", sd=dt.datetime(2010,1,1), ed=dt.datetime(2011,12,31), sv = 100000) # testing phase\n",
    "\n",
    "The input parameters are:\n",
    "\n",
    "* verbose: if False do not generate any output\n",
    "* impact: The market impact of each transaction.\n",
    "* symbol: the stock symbol to train on\n",
    "* sd: A datetime object that represents the start date\n",
    "* ed: A datetime object that represents the end date\n",
    "* sv: Start value of the portfolio\n",
    "\n",
    "The output result is:\n",
    "\n",
    "* df_trades: A data frame whose values represent trades for each day. Legal values are +1000.0 indicating a BUY of 1000 shares, -1000.0 indicating a SELL of 1000 shares, and 0.0 indicating NOTHING. Values of +2000 and -2000 for trades are also legal when switching from long to short or short to long so long as net holdings are constrained to -1000, 0, and 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Implement a StrategyLearner that trains a QLearner for trading a symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import datetime as dt\n",
    "\n",
    "from util import create_df_benchmark, get_data\n",
    "from strategyLearner import strategyLearner\n",
    "from marketsim import compute_portvals_single_symbol, market_simulator\n",
    "from indicators import get_momentum, get_sma, get_sma_indicator, compute_bollinger_value, get_RSI, plot_cum_return,  plot_momentum, plot_sma_indicator, plot_rsi_indicator, plot_momentum_sma_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_val = 100000\n",
    "symbol = \"JPM\"\n",
    "commission = 0.00\n",
    "impact = 0.0\n",
    "num_shares = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-sample performance\n",
    "\n",
    "Show the performances of portfolio and benchmark in the in-sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and end dates for this period.\n",
    "start_d = dt.datetime(2008, 1, 1)\n",
    "end_d = dt.datetime(2009, 12, 31)\n",
    "\n",
    "# Get benchmark data\n",
    "benchmark_prices = get_data([symbol], pd.date_range(start_d, end_d), addSPY=False).dropna()\n",
    "\n",
    "# Create benchmark data: Benchmark is a portfolio starting with $100,000, investing in 1000 shares of symbol and holding that position\n",
    "df_benchmark_trades = create_df_benchmark(symbol, start_d, end_d, num_shares)\n",
    "\n",
    "#print (df_benchmark_trades)\n",
    "\n",
    "# Train and test a StrategyLearner\n",
    "# Set verbose to True will print out and plot the cumulative return for each training epoch\n",
    "stl = strategyLearner(num_shares=num_shares, impact=impact, \n",
    "                      commission=commission, verbose=True,\n",
    "                      num_states=3000, num_actions=3)\n",
    "stl.add_evidence(symbol=symbol, start_val=start_val, \n",
    "                 start_date=start_d, end_date=end_d)\n",
    "df_trades = stl.test_policy(symbol=symbol, start_date=start_d,\n",
    "                            end_date=end_d)\n",
    "#print (df_trades)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training market simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve performance stats via a market simulator\n",
    "print (\"Performances during training period for {}\".format(symbol))\n",
    "print (\"Date Range: {} to {}\".format(start_d, end_d))\n",
    "orders_count, sharpe_ratio, cum_ret, std_daily_ret, avg_daily_ret, final_value = market_simulator(df_trades, df_benchmark_trades, symbol=symbol, \n",
    "                 start_val=start_val, commission=commission, impact=impact, title=\"Portfolio Value\", xtitle=\"Dates\", ytitle=\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of sample performance\n",
    "\n",
    "Show the performances of portfolio and benchmark in the out of sample period. Use the same StrategyLearner trained above and retrieve a trades dataframe via test_policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and end dates for this period.\n",
    "start_d = dt.datetime(2010, 1, 1)\n",
    "end_d = dt.datetime(2011, 12, 31)\n",
    "\n",
    "# Get benchmark data\n",
    "benchmark_prices = get_data([symbol], pd.date_range(start_d, end_d), addSPY=False).dropna()\n",
    "\n",
    "# Create benchmark data: Benchmark is a portfolio starting with $100,000, investing in 1000 shares of symbol and holding that position\n",
    "df_benchmark_trades = create_df_benchmark(symbol, start_d, end_d, num_shares)\n",
    "\n",
    "#print (df_benchmark_trades)\n",
    "\n",
    "# Test a StrategyLearner\n",
    "# Use the same StrategyLearner trained above and retrieve a trades dataframe via test_policy\n",
    "df_trades = stl.test_policy(symbol=symbol, start_date=start_d,\n",
    "                            end_date=end_d)\n",
    "#print (df_trades)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test market simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve performance stats via a market simulator\n",
    "print (\"Performances during test period for {}\".format(symbol))\n",
    "print (\"Date Range: {} to {}\".format(start_d, end_d))\n",
    "orders_count, sharpe_ratio, cum_ret, std_daily_ret, avg_daily_ret, final_value = market_simulator(df_trades, df_benchmark_trades, symbol=symbol, \n",
    "                 start_val=start_val, commission=commission, impact=impact, title=\"Portfolio Value\", xtitle=\"Dates\", ytitle=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Describe the steps you took to frame the trading problem as a learning problem for your learner. What are your indicators? Did you adjust the data in any way (dicretization, standardization)? Why or why not?\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Get adjusted close prices for symbol.\n",
    "2. Set the QLearner parameters:\n",
    "    * num_shares: The number of shares that can be traded in one order\n",
    "    * epochs: The number of times to train the QLearner\n",
    "    * num_steps: The number of steps used in getting thresholds for the\n",
    "    discretization process. It is the number of groups to put data into.\n",
    "3. Compute technical indicators and use them as features to be fed into a Q-learner.\n",
    "    * Momentum\n",
    "    * SMA indicator\n",
    "    * RSI indicator\n",
    "4. Get features and thresholds.\n",
    "5. Define states, actions, and rewards. \n",
    "    * States are combinations of our features. \n",
    "    * Actions are buy, sell, do nothing. \n",
    "    * Rewards:\n",
    "        Calculate the daily reward as a percentage change in prices: \n",
    "        - Position is long: if the price goes up (curr_price > prev_price),\n",
    "          we get a positive reward; otherwise, we get a negative reward\n",
    "        - Position is short: if the price goes down, we get a positive reward;\n",
    "        otherwise, we a negative reward\n",
    "        - Position is cash: we get no reward\n",
    "6. Set initial position holding to nothing.\n",
    "7. Create a series that captures order signals based on actions taken.\n",
    "8. Iterate over the data by date.\n",
    "    * Discretize features and return a state. Get a state; add 1 to position so that states >= 0.\n",
    "    * On the first day, get an action without updating the Q-table.\n",
    "    * On the last day, close any open positions.\n",
    "    * Add new_pos to orders.\n",
    "    * Update current position.\n",
    "9. Create a trade dataframe.\n",
    "10. Training: Choose the training period and you iterate over that training period and update your Q-table on each iteration. When you reach the end of that training period you backtest to see how good the model is and you go back and repeat, until the model quits getting better.\n",
    "Once it's converged you stop, you've got your model.\n",
    "11. Testing the model: You just backtest it on later data.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and end dates for this period.\n",
    "start_d = dt.datetime(2008, 1, 1)\n",
    "end_d = dt.datetime(2009, 12, 31)\n",
    "\n",
    "# Set dates\n",
    "dates = pd.date_range(start_d, end_d)\n",
    "\n",
    "# Get adjusted close prices for symbol\n",
    "df = get_data([symbol], dates, addSPY=False)\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# Normalize the prices Dataframe\n",
    "normed = pd.DataFrame()\n",
    "for column in df:\n",
    "    normed[column] = df[column].values / df[column].iloc[0];\n",
    "\n",
    "# 2. Compute momentum\n",
    "sym_mom = get_momentum(normed[column], window=10)\n",
    "\n",
    "# 3. Plot raw JPM values and Momentum\n",
    "plot_momentum(df.index, normed[column], sym_mom, \"Momentum Indicator\", (12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMA indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and end dates for this period.\n",
    "start_d = dt.datetime(2008, 1, 1)\n",
    "end_d = dt.datetime(2009, 12, 31)\n",
    "\n",
    "# Set dates\n",
    "dates = pd.date_range(start_d, end_d)\n",
    "\n",
    "# Get adjusted close prices for symbol\n",
    "df = get_data([symbol], dates, addSPY=False)\n",
    "df = df.dropna()\n",
    "\n",
    "# Normalize the prices Dataframe\n",
    "normed = pd.DataFrame()\n",
    "for column in df:\n",
    "    normed[column] = df[column].values / df[column].iloc[0];\n",
    "\n",
    "# Compute SMA\n",
    "sma_JPM, q = get_sma(normed[column], window=10)\n",
    "\n",
    "# Plot symbol values, SMA and SMA quality\n",
    "plot_sma_indicator(dates, df.index, normed[column], sma_JPM, q, \"Simple Moving Average (SMA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum/SMA cross indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and end dates for this period.\n",
    "start_d = dt.datetime(2008, 1, 1)\n",
    "end_d = dt.datetime(2009, 12, 31)\n",
    "\n",
    "# Set dates\n",
    "dates = pd.date_range(start_d, end_d)\n",
    "\n",
    "# Get adjusted close prices for symbol\n",
    "df = get_data([symbol], dates, addSPY=False)\n",
    "df = df.dropna()\n",
    "\n",
    "# Compute momentum\n",
    "sym_mom = get_momentum(normed[column], window=10)\n",
    "\n",
    "# Compute SMA\n",
    "sma_JPM, q = get_sma(normed[column], window=10)\n",
    "\n",
    "# Plot symbol values, SMA and Momentum\n",
    "plot_momentum_sma_indicator(dates, df.index, normed[column], sma_JPM, sym_mom, \"Momentum/SMA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSI indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and end dates for this period.\n",
    "start_d = dt.datetime(2008, 1, 1)\n",
    "end_d = dt.datetime(2009, 12, 31)\n",
    "\n",
    "# Set dates\n",
    "dates = pd.date_range(start_d, end_d)\n",
    "\n",
    "# Get adjusted close prices for symbol\n",
    "df = get_data([symbol], dates, addSPY=False)\n",
    "df = df.dropna()\n",
    "\n",
    "# 1. Compute RSI\n",
    "rsi_JPM = get_RSI(df['JPM'])\n",
    "\n",
    "# 2. Plot RSI\n",
    "plot_rsi_indicator(dates, df.index, df['JPM'], rsi_JPM, window=14, \n",
    "                   title=\"RSI Indicator\", fig_size=(12, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Write a PDF report describing your system. The centerpiece of your report should be the description of how you utilized your learner to determine trades"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

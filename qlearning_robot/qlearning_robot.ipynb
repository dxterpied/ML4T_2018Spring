{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Qlearning robot\n",
    "ML for trading Udacity Course exercise\n",
    "\n",
    "More info:\n",
    "https://quantsoftware.gatech.edu/Manual_strategy\n",
    "\n",
    "A transcription of the Udacity Course lectures can be find on https://docs.google.com/document/d/1ELqlnuTSdc9-MDHOkV0uvSY4RmI1eslyQlU9DgOY_jc/edit?usp=sharing\n",
    "\n",
    "Kairoart 2018\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this project you will implement the Q-Learning and Dyna-Q solutions to the reinforcement learning problem. You will apply them to a navigation problem in this project. In a later project you will apply them to trading. The reason for working with the navigation problem first is that, as you will see, navigation is an easy problem to work with and understand. Note that your Q-Learning code really shouldn't care which problem it is solving. The difference is that you need to wrap the learner in different code that frames the problem for the learner as necessary.\n",
    "\n",
    "For the navigation problem we have created testqlearner.py that automates testing of your Q-Learner in the navigation problem.\n",
    "\n",
    "Overall, your tasks for this project include:\n",
    "\n",
    "    * Code a Q-Learner\n",
    "    * Code the Dyna-Q feature of Q-Learning\n",
    "    * Test/debug the Q-Learner in navigation problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The navigation task\n",
    "\n",
    "The navigation task takes place in a 10 x 10 grid world. The particular environment is expressed in a CSV file of integers, where the value in each position is interpreted as follows:\n",
    "\n",
    "0: blank space.\n",
    "\n",
    "1: an obstacle.\n",
    "\n",
    "2: the starting location for the robot.\n",
    "\n",
    "3: the goal location.\n",
    "\n",
    "5: quicksand. \n",
    "\n",
    "An example navigation problem (world01.csv) is shown below. Following python conventions, [0,0] is upper left, or northwest corner, [9,9] lower right or southeast corner. Rows are north/south, columns are east/west.\n",
    "\n",
    "3,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "0,0,1,1,1,1,1,0,0,0\n",
    "\n",
    "0,5,1,0,0,0,1,0,0,0\n",
    "\n",
    "0,5,1,0,0,0,1,0,0,0\n",
    "\n",
    "0,0,1,0,0,0,1,0,0,0\n",
    "\n",
    "0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "0,0,0,0,2,0,0,0,0,0\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "In this example the robot starts at the bottom center, and must navigate to the top left. Note that a wall of obstacles blocks its path, and there is some quicksand along the left side. The objective is for the robot to learn how to navigate from the starting location to the goal with the highest total reward. We define the reward for each step as:\n",
    "\n",
    "-1 if the robot moves to an empty or blank space, or attempts to move into a wall\n",
    "\n",
    "-100 if the robot moves to a quicksand space\n",
    "\n",
    "1 if the robot moves to the goal space\n",
    "\n",
    "### Assess\n",
    "Overall, we will assess the performance of a policy as the median reward it incurs to travel from the start to the goal (higher reward is better). We assess a learner in terms of the reward it converges to over a given number of training epochs (trips from start to goal). The problem includes random actions. So, for example, if the learner responds with a \"move north\" action, there is some probability that the robot will actually move in a different direction. For this reason, the \"wise\" learner develops policies that keep the robot well away from quicksand. We map this problem to a reinforcement learning problem as follows:\n",
    "\n",
    "**State:** The state is the location of the robot, it is computed (discretized) as: column location * 10 + row location.\n",
    "\n",
    "**Actions:** There are 4 possible actions, 0: move north, 1: move east, 2: move south, 3: move west.\n",
    "\n",
    "**R:** The reward is as described above.\n",
    "\n",
    "**T:** The transition matrix can be inferred from the CSV map and the actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import random as rand\n",
    "import time  \n",
    "import math  \n",
    "import QLearner as ql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the map  \n",
    "def printmap(data):  \n",
    "    \n",
    "    print(\"--------------------\")\n",
    "    for row in range(0, data.shape[0]):  \n",
    "        for col in range(0, data.shape[1]):  \n",
    "            print(data[row, col])\n",
    "            if data[row,col] == str(0): # Empty space  \n",
    "                data[row,col] = \" \",  \n",
    "            if data[row,col] == str(1): # Obstacle  \n",
    "                data[row,col] = \"O\",  \n",
    "            if data[row,col] == str(2): # El roboto  \n",
    "                data[row,col] = \"*\",   \n",
    "            if data[row,col] == str(3): # Goal  \n",
    "                data[row,col] = \"X\",   \n",
    "            if data[row,col] == str(4): # Trail  \n",
    "                data[row,col] = \".\",   \n",
    "            if data[row,col] == str(5): # Quick sand  \n",
    "                data[row,col] = \"-\", \n",
    "            if data[row,col] == str(6): # Stepped in quicksand  \n",
    "                data[row,col] = \" @\",  \n",
    "    print(data)  \n",
    "    print(\"--------------------\")\n",
    "\n",
    "# find where the robot is in the map  \n",
    "def getrobotpos(data):  \n",
    "    print(data)\n",
    "    R = -999  \n",
    "    C = -999  \n",
    "    for row in range(0, data.shape[0]):  \n",
    "        for col in range(0, data.shape[1]):  \n",
    "            if data[row,col] == 2:  \n",
    "                C = col  \n",
    "                R = row  \n",
    "    if (R+C)<0:  \n",
    "        print(\"warning: start location not defined\")\n",
    "    return R, C  \n",
    " \n",
    "# find where the goal is in the map \n",
    "def getgoalpos(data):  \n",
    "    R = -999  \n",
    "    C = -999  \n",
    "    for row in range(0, data.shape[0]):  \n",
    "        for col in range(0, data.shape[1]):  \n",
    "            if data[row,col] == 3: \n",
    "                C = col  \n",
    "                R = row  \n",
    "    if (R+C)<0:  \n",
    "        print(\"warning: goal location not defined\")\n",
    "    return (R, C)  \n",
    "\n",
    "# move the robot and report reward \n",
    "def movebot(data,oldpos,a):  \n",
    "    testr, testc = oldpos  \n",
    "\n",
    "    randomrate = 0.20 # how often do we move randomly  \n",
    "    quicksandreward = -100 # penalty for stepping on quicksand  \n",
    "\n",
    "    # decide if we're going to ignore the action and  \n",
    "    # choose a random one instead  \n",
    "    if rand.uniform(0.0, 1.0) <= randomrate: # going rogue  \n",
    "        a = rand.randint(0,3) # choose the random direction  \n",
    "\n",
    "    # update the test location \n",
    "    if a == 0: #north  \n",
    "        testr = testr - 1  \n",
    "    elif a == 1: #east  \n",
    "        testc = testc + 1 \n",
    "    elif a == 2: #south  \n",
    "        testr = testr + 1\n",
    "    elif a == 3: #west  \n",
    "        testc = testc - 1\n",
    "\n",
    "    reward = -1 # default reward is negative one \n",
    "    # see if it is legal. if not, revert \n",
    "    if testr < 0: # off the map \n",
    "        testr, testc = oldpos \n",
    "    elif testr >= data.shape[0]: # off the map \n",
    "        testr, testc = oldpos \n",
    "    elif testc < 0: # off the map \n",
    "        testr, testc = oldpos  \n",
    "    elif testc >= data.shape[1]: # off the map \n",
    "        testr, testc = oldpos  \n",
    "    elif data[testr, testc] == 1: # it is an obstacle \n",
    "        testr, testc = oldpos  \n",
    "    elif data[testr, testc] == 5: # it is quicksand  \n",
    "        reward = quicksandreward  \n",
    "        data[testr, testc] = 6 # mark the event \n",
    "    elif data[testr, testc] == 6: # it is still quicksand  \n",
    "        reward = quicksandreward \n",
    "        data[testr, testc] = 6 # mark the event \n",
    "    elif data[testr, testc] == 3:  # it is the goal  \n",
    "        reward = 1 # for reaching the goal \n",
    " \n",
    "    return (testr, testc), reward #return the new, legal location \n",
    "  \n",
    "# convert the location to a single integer \n",
    "def discretize(pos):  \n",
    "    return pos[0]*10 + pos[1]  \n",
    "\n",
    "def test(map, epochs, learner, verbose):  \n",
    "    # each epoch involves one trip to the goal \n",
    "    startpos = getrobotpos(map) #find where the robot starts  \n",
    "    goalpos = getgoalpos(map) #find where the goal is \n",
    "    scores = np.zeros((epochs,1))  \n",
    "    for epoch in range(1,epochs+1): \n",
    "        total_reward = 0 \n",
    "        data = map.copy() \n",
    "        robopos = startpos \n",
    "        state = discretize(robopos) #convert the location to a state \n",
    "        action = learner.query_set_state(state) #set the state and get first action  \n",
    "        count = 0  \n",
    "        while (robopos != goalpos) & (count<10000): \n",
    "\n",
    "            #move to new location according to action and then get a new action \n",
    "            newpos, stepreward = movebot(data,robopos,action)  \n",
    "            if newpos == goalpos:  \n",
    "                r = 1 # reward for reaching the goal \n",
    "            else:  \n",
    "                r = stepreward # negative reward for not being at the goal \n",
    "            state = discretize(newpos)  \n",
    "            action = ql.QLearner.query(state,r) \n",
    "\n",
    "            if data[robopos] != 6: \n",
    "                data[robopos] = 4 # mark where we've been for map printing \n",
    "            if data[newpos] != 6: \n",
    "                data[newpos] = 2 # move to new location \n",
    "            robopos = newpos # update the location  \n",
    "            #if verbose: time.sleep(1) \n",
    "            total_reward += stepreward  \n",
    "            count = count + 1  \n",
    "        if count == 100000: \n",
    "            print(\"timeout\")\n",
    "        if verbose: printmap(data)  \n",
    "        if verbose: \n",
    "            print(epoch, total_reward)\n",
    "        scores[epoch-1,0] = total_reward  \n",
    "    return np.median(scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_code(): \n",
    "    # Get values from cvs file\n",
    "    df = pd.read_csv(\"testworlds/world01.csv\", sep=\",\", header=None)\n",
    "\n",
    "    # Convert dataframe to array\n",
    "    data = df.values\n",
    "    \n",
    "    # Make a copy so we can revert to the original map later\n",
    "    originalmap = data.copy() \n",
    "    \n",
    "    # Print lots of debug stuff if True\n",
    "    verbose = False  \n",
    "    if verbose: \n",
    "        printmap(data)        \n",
    "    \n",
    "    rand.seed(5)  \n",
    "\n",
    "    ######## run non-dyna test ######## \n",
    "    learner = ql.QLearner(num_states=100,\\\n",
    "        num_actions = 4, \\\n",
    "        alpha = 0.2, \\\n",
    "        gamma = 0.9, \\\n",
    "        rar = 0.98, \\\n",
    "        radr = 0.999, \\\n",
    "        dyna = 0, \\\n",
    "        verbose=False) #initialize the learner  \n",
    "    epochs = 500 \n",
    "\n",
    "    total_reward = test(data, epochs, learner, verbose) \n",
    "    print (epochs, \"median total_reward\" , total_reward) \n",
    "\n",
    "    non_dyna_score = total_reward  \n",
    "\n",
    "    ######## run dyna test ########  \n",
    "    learner = ql.QLearner(num_states=100,\\\n",
    "        num_actions = 4, \\\n",
    "        alpha = 0.2, \\\n",
    "        gamma = 0.9, \\\n",
    "        rar = 0.5, \\\n",
    "        radr = 0.99, \\\n",
    "        dyna = 200, \\\n",
    "        verbose=False) #initialize the learner \n",
    "    epochs = 50  \n",
    "    data = originalmap.copy()  \n",
    "    total_reward = test(data, epochs, learner, verbose) \n",
    "    print(epochs, \"median total_reward\" , total_reward)\n",
    "    dyna_score = total_reward  \n",
    "    \n",
    "    print (\"results for\", filename  )\n",
    "    print (\"non_dyna_score:\", non_dyna_score)\n",
    "    print (\"dyna_score    :\", dyna_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 0 0 0]\n",
      " [0 5 1 0 0 0 1 0 0 0]\n",
      " [0 5 1 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "query() missing 1 required positional argument: 'r'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-444c778fe7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-121-ccd741e7307f>\u001b[0m in \u001b[0;36mtest_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"median total_reward\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-3ccfbd346591>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(map, epochs, learner, verbose)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepreward\u001b[0m \u001b[0;31m# negative reward for not being at the goal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrobopos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: query() missing 1 required positional argument: 'r'"
     ]
    }
   ],
   "source": [
    "test_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

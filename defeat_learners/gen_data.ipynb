{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defeat learners\n",
    "ML for trading Udacity Course exercise\n",
    "\n",
    "More info:\n",
    "https://quantsoftware.gatech.edu/Defeat_learners\n",
    "\n",
    "A transcription of the Udacity Course lectures can be find on https://docs.google.com/document/d/1ELqlnuTSdc9-MDHOkV0uvSY4RmI1eslyQlU9DgOY_jc/edit?usp=sharing\n",
    "\n",
    "Kairosart 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project you will generate data that you believe will work better for one learner than another. This will test your understanding of the strengths and weaknesses of various learners. The two learners you should aim your datasets at are:\n",
    "\n",
    "    * A decision tree learner with leaf_size = 1 (DTLearner). Note that for testing purposes we will use our implementation of DTLearner\n",
    "    * A LinRegLearner provided as part of the repo.\n",
    "\n",
    "Your data generation should use a random number generator as part of its data generation process. We will pass your generators a random number seed. Whenever the seed is the same you should return exactly the same data set. Different seeds should result in different data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import math  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best4LinReg(seed=np.random.randint(1000000)):\n",
    "    \"\"\"This function should return a dataset (X and Y) that will work\n",
    "    better for linear regression than decision trees\n",
    "    Parameters: \n",
    "    seed: Random integer used to initialize the pseudo-random number generator. \n",
    "    Whenever the seed is the same, the same data set will be returned. \n",
    "    Different seeds should result in different data sets.\n",
    "    Returns: \n",
    "    X: A numpy ndarray of X values\n",
    "    Y: A numpy 1D array of Y values\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # X and Y should each contain from 10 to 1000 rows\n",
    "    num_rows = np.random.randint(10, 1001)\n",
    "\n",
    "    # X should have from 2 to 10 columns\n",
    "    num_X_cols = np.random.randint(2, 11)\n",
    "\n",
    "    X = np.random.normal(size=(num_rows, num_X_cols))\n",
    "    Y = np.zeros(num_rows)\n",
    "    for col in range(num_X_cols):\n",
    "        Y += X[:, col]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def best4DT(seed=np.random.randint(1000000)):\n",
    "    \"\"\"This function should return a dataset (X and Y) that will work\n",
    "    better for decision trees than linear regression\n",
    "    Parameters: \n",
    "    seed: Random seed used to initialize the pseudo-random number generator. \n",
    "    Whenever the seed is the same, the same data set will be returned. \n",
    "    Different seeds should result in different data sets.\n",
    "    Returns: \n",
    "    X: A numpy ndarray of X values\n",
    "    Y: A numpy 1D array of Y values\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # X and Y should each contain from 10 to 1000 rows\n",
    "    num_rows = np.random.randint(10, 1001)\n",
    "\n",
    "    # X should have from 2 to 10 columns\n",
    "    num_X_cols = np.random.randint(2, 11)\n",
    "\n",
    "    X = np.random.normal(size=(num_rows, num_X_cols))\n",
    "    Y = np.zeros(num_rows)\n",
    "    for col in range(num_X_cols):\n",
    "        Y += X[:, col] ** 2\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data to fool learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, Y1 = best4LinReg(seed = 5)\n",
    "X2, Y2 = best4DT(seed = 5)\n",
    "print(Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deductions:\n",
    "\n",
    "    * Does either dataset returned contain fewer or more than the allowed number of samples?\n",
    "    * Does either dataset returned contain fewer or more than the allowed number of dimensions in X?\n",
    "    * When the seed is the same does the best4LinReg dataset generator return the same data?\n",
    "    * When the seed is the same does the best4DT dataset generator return the same data?\n",
    "    * When the seed is different does the best4LinReg dataset generator return different data?\n",
    "    * When the seed is different does the best4DT dataset generator return different data?\n",
    "    * Does the code attempt to import a learner? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  877 877\n",
      "Dimensions:  (877, 8) (877, 8)\n",
      "When the seed is the same does the best4LinReg dataset generator return the same data.\n",
      "When the seed is the same does the best4DT dataset generator return the same data.\n"
     ]
    }
   ],
   "source": [
    "# Number of samples\n",
    "print(\"Samples: \", X1.shape[0], X2.shape[0]) \n",
    "\n",
    "# Dimensions\n",
    "print(\"Dimensions: \", X1.shape, X2.shape)\n",
    "\n",
    "# best4LinReg dataset generator\n",
    "print(\"When the seed is the same does the best4LinReg dataset generator return the same data.\")\n",
    "\n",
    "# best4DT dataset generator\n",
    "print(\"When the seed is the same does the best4DT dataset generator return the same data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTLearner\n",
    "\n",
    "A simple wrapper for Decision Tree regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTLearner(object):\n",
    "\n",
    "    def __init__(self, leaf_size=1, verbose=False, tree=None):\n",
    "        self.leaf_size = leaf_size\n",
    "        self.verbose = verbose\n",
    "        self.tree = deepcopy(tree)\n",
    "        if verbose:\n",
    "            self.get_learner_info()\n",
    "        \n",
    "\n",
    "    def __build_tree(self, dataX, dataY, rootX=[], rootY=[]):\n",
    "        \"\"\"Builds the Decision Tree recursively by choosing the best feature to split on and \n",
    "        the splitting value. The best feature has the highest absolute correlation with dataY. \n",
    "        If all features have the same absolute correlation, choose the first feature. The \n",
    "        splitting value is the median of the data according to the best feature. \n",
    "        If the best feature doesn't split the data into two groups, choose the second best \n",
    "        one and so on; if none of the features does, return leaf\n",
    "        Parameters:\n",
    "        dataX: A numpy ndarray of X values at each node\n",
    "        dataY: A numpy 1D array of Y values at each node\n",
    "        rootX: A numpy ndarray of X values at the parent/root node of the current one\n",
    "        rootY: A numpy 1D array of Y values at the parent/root node of the current one\n",
    "        \n",
    "        Returns:\n",
    "        tree: A numpy ndarray. Each row represents a node and four columns are feature indices \n",
    "        (int type; index for a leaf is -1), splitting values, and starting rows, from the current \n",
    "        root, for its left and right subtrees (if any)\n",
    "        \"\"\"\n",
    "        # Get the number of samples (rows) and features (columns) of dataX\n",
    "        num_samples = dataX.shape[0]\n",
    "        num_feats = dataX.shape[1]\n",
    "        \n",
    "        \n",
    "        # If there is no sample left, return the most common value from the root of current node\n",
    "        if num_samples == 0:\n",
    "            return np.array([-1, Counter(rootY).most_common(1)[0][0], np.nan, np.nan])\n",
    "\n",
    "        # If there are <= leaf_size samples or all data in dataY are the same, return leaf\n",
    "        if num_samples <= self.leaf_size or len(pd.unique(dataY)) == 1:\n",
    "            return np.array([-1, Counter(dataY).most_common(1)[0][0], np.nan, np.nan])\n",
    "    \n",
    "        avail_feats_for_split = list(range(num_feats))\n",
    "\n",
    "        # Get a list of tuples of features and their correlations with dataY\n",
    "        feats_corrs = []\n",
    "        for feat_i in range(num_feats):\n",
    "            abs_corr = abs(pearsonr(dataX[:, feat_i], dataY)[0])\n",
    "            feats_corrs.append((feat_i, abs_corr))\n",
    "        \n",
    "        # Sort the list in descending order by correlation\n",
    "        feats_corrs = sorted(feats_corrs, key=itemgetter(1), reverse=True)\n",
    "\n",
    "        # Choose the best feature, if any, by iterating over feats_corrs\n",
    "        feat_corr_i = 0\n",
    "        while len(avail_feats_for_split) > 0:\n",
    "            best_feat_i = feats_corrs[feat_corr_i][0]\n",
    "            best_abs_corr = feats_corrs[feat_corr_i][1]\n",
    "\n",
    "            # Split the data according to the best feature\n",
    "            split_val = np.median(dataX[:, best_feat_i])\n",
    "\n",
    "            # Logical arrays for indexing\n",
    "            left_index = dataX[:, best_feat_i] <= split_val\n",
    "            right_index = dataX[:, best_feat_i] > split_val\n",
    "\n",
    "            # If we can split the data into two groups, then break out of the loop            \n",
    "            if len(np.unique(left_index)) != 1:\n",
    "                break\n",
    "            \n",
    "            avail_feats_for_split.remove(best_feat_i)\n",
    "            feat_corr_i += 1\n",
    "        \n",
    "        # If we complete the while loop and run out of features to split, return leaf\n",
    "        if len(avail_feats_for_split) == 0:\n",
    "            return np.array([-1, Counter(dataY).most_common(1)[0][0], np.nan, np.nan])\n",
    "\n",
    "        # Build left and right branches and the root                    \n",
    "        lefttree = self.__build_tree(dataX[left_index], dataY[left_index], dataX, dataY)\n",
    "        righttree = self.__build_tree(dataX[right_index], dataY[right_index], dataX, dataY)\n",
    "\n",
    "        # Set the starting row for the right subtree of the current root\n",
    "        if lefttree.ndim == 1:\n",
    "            righttree_start = 2 # The right subtree starts 2 rows down\n",
    "        elif lefttree.ndim > 1:\n",
    "            righttree_start = lefttree.shape[0] + 1\n",
    "        root = np.array([best_feat_i, split_val, 1, righttree_start])\n",
    "\n",
    "        return np.vstack((root, lefttree, righttree))\n",
    "    \n",
    "\n",
    "    def __tree_search(self, point, row):\n",
    "        \"\"\"A private function to be used with query. It recursively searches \n",
    "        the decision tree matrix and returns a predicted value for point\n",
    "        Parameters:\n",
    "        point: A numpy 1D array of test query\n",
    "        row: The row of the decision tree matrix to search\n",
    "    \n",
    "        Returns \n",
    "        pred: The predicted value\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the feature on the row and its corresponding splitting value\n",
    "        feat, split_val = self.tree[row, 0:2]\n",
    "        \n",
    "        # If splitting value of feature is -1, we have reached a leaf so return it\n",
    "        if feat == -1:\n",
    "            return split_val\n",
    "\n",
    "        # If the corresponding feature's value from point <= split_val, go to the left tree\n",
    "        elif point[int(feat)] <= split_val:\n",
    "            pred = self.__tree_search(point, row + int(self.tree[row, 2]))\n",
    "\n",
    "        # Otherwise, go to the right tree\n",
    "        else:\n",
    "            pred = self.__tree_search(point, row + int(self.tree[row, 3]))\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "    def addEvidence(self, dataX, dataY):\n",
    "        \"\"\"Add training data to learner\n",
    "        Parameters:\n",
    "        dataX: A numpy ndarray of X values of data to add\n",
    "        dataY: A numpy 1D array of Y training values\n",
    "        Returns: An updated tree matrix for DTLearner\n",
    "        \"\"\"\n",
    "\n",
    "        new_tree = self.__build_tree(dataX, dataY)\n",
    "\n",
    "        # If self.tree is currently None, simply assign new_tree to it\n",
    "        if self.tree is None:\n",
    "            self.tree = new_tree\n",
    "\n",
    "        # Otherwise, append new_tree to self.tree\n",
    "        else:\n",
    "            self.tree = np.vstack((self.tree, new_tree))\n",
    "        \n",
    "        # If there is only a single row, expand tree to a numpy ndarray for consistency\n",
    "        if len(self.tree.shape) == 1:\n",
    "            self.tree = np.expand_dims(self.tree, axis=0)\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.get_learner_info()\n",
    "        \n",
    "        \n",
    "    def query(self, points):\n",
    "        \"\"\"Estimates a set of test points given the model we built\n",
    "        \n",
    "        Parameters:\n",
    "        points: A numpy ndarray of test queries\n",
    "        Returns: \n",
    "        preds: A numpy 1D array of the estimated values\n",
    "        \"\"\"\n",
    "\n",
    "        preds = []\n",
    "        for point in points:\n",
    "            preds.append(self.__tree_search(point, row=0))\n",
    "        return np.asarray(preds)\n",
    "\n",
    "\n",
    "    def get_learner_info(self):\n",
    "        print (\"Info about this Decision Tree Learner:\")\n",
    "        print (\"leaf_size =\", self.leaf_size)\n",
    "        if self.tree is not None:\n",
    "            print (\"tree shape =\", self.tree.shape)\n",
    "            print (\"tree as a matrix:\")\n",
    "            # Create a dataframe from tree for a user-friendly view\n",
    "            df_tree = pd.DataFrame(self.tree, columns=[\"factor\", \"split_val\", \"left\", \"right\"])\n",
    "            df_tree.index.name = \"node\"\n",
    "            print (df_tree)\n",
    "        else:\n",
    "            print (\"Tree has no data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some data to test the DTLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Decision Tree Learner\n",
      "\n",
      "Info about this Decision Tree Learner:\n",
      "leaf_size = 1\n",
      "Tree has no data\n",
      "\n",
      "Add data\n",
      "Info about this Decision Tree Learner:\n",
      "leaf_size = 1\n",
      "tree shape = (15, 4)\n",
      "tree as a matrix:\n",
      "      factor  split_val  left  right\n",
      "node                                \n",
      "0        2.0     9.9000   1.0    8.0\n",
      "1        2.0     9.2500   1.0    4.0\n",
      "2        0.0     0.7475   1.0    2.0\n",
      "3       -1.0     3.0000   NaN    NaN\n",
      "4       -1.0     4.0000   NaN    NaN\n",
      "5        0.0     0.6475   1.0    2.0\n",
      "6       -1.0     6.0000   NaN    NaN\n",
      "7       -1.0     5.0000   NaN    NaN\n",
      "8        0.0     0.4100   1.0    4.0\n",
      "9        0.0     0.2900   1.0    2.0\n",
      "10      -1.0     8.0000   NaN    NaN\n",
      "11      -1.0     6.0000   NaN    NaN\n",
      "12       0.0     0.6125   1.0    2.0\n",
      "13      -1.0     7.0000   NaN    NaN\n",
      "14      -1.0     5.0000   NaN    NaN\n",
      "\n",
      "Create another tree learner from an existing tree\n",
      "Info about this Decision Tree Learner:\n",
      "leaf_size = 1\n",
      "tree shape = (15, 4)\n",
      "tree as a matrix:\n",
      "      factor  split_val  left  right\n",
      "node                                \n",
      "0        2.0     9.9000   1.0    8.0\n",
      "1        2.0     9.2500   1.0    4.0\n",
      "2        0.0     0.7475   1.0    2.0\n",
      "3       -1.0     3.0000   NaN    NaN\n",
      "4       -1.0     4.0000   NaN    NaN\n",
      "5        0.0     0.6475   1.0    2.0\n",
      "6       -1.0     6.0000   NaN    NaN\n",
      "7       -1.0     5.0000   NaN    NaN\n",
      "8        0.0     0.4100   1.0    4.0\n",
      "9        0.0     0.2900   1.0    2.0\n",
      "10      -1.0     8.0000   NaN    NaN\n",
      "11      -1.0     6.0000   NaN    NaN\n",
      "12       0.0     0.6125   1.0    2.0\n",
      "13      -1.0     7.0000   NaN    NaN\n",
      "14      -1.0     5.0000   NaN    NaN\n",
      "Info about this Decision Tree Learner:\n",
      "leaf_size = 1\n",
      "Tree has no data\n",
      "Info about this Decision Tree Learner:\n",
      "leaf_size = 1\n",
      "tree shape = (7, 4)\n",
      "tree as a matrix:\n",
      "      factor  split_val  left  right\n",
      "node                                \n",
      "0        2.0    10.0000   1.0    6.0\n",
      "1        0.0     0.3200   1.0    2.0\n",
      "2       -1.0     6.0000   NaN    NaN\n",
      "3        0.0     0.6725   1.0    2.0\n",
      "4       -1.0     3.0000   NaN    NaN\n",
      "5       -1.0     5.0000   NaN    NaN\n",
      "6       -1.0     8.0000   NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "print (\"This is a Decision Tree Learner\\n\")\n",
    "\n",
    "# Some data to test the DTLearner\n",
    "x0 = np.array([0.885, 0.725, 0.560, 0.735, 0.610, 0.260, 0.500, 0.320])\n",
    "x1 = np.array([0.330, 0.390, 0.500, 0.570, 0.630, 0.630, 0.680, 0.780])\n",
    "x2 = np.array([9.100, 10.900, 9.400, 9.800, 8.400, 11.800, 10.500, 10.000])\n",
    "x = np.array([x0, x1, x2]).T\n",
    "\n",
    "y = np.array([4.000, 5.000, 6.000, 5.000, 3.000, 8.000, 7.000, 6.000])\n",
    "\n",
    "\n",
    "# Create a tree learner from given train X and y\n",
    "dtl = DTLearner(verbose=True, leaf_size=1)\n",
    "print (\"\\nAdd data\")\n",
    "dtl.addEvidence(x, y)\n",
    "\n",
    "print (\"\\nCreate another tree learner from an existing tree\")\n",
    "dtl2 = DTLearner(tree=dtl.tree)\n",
    "\n",
    "# dtl2 should have the same tree as dtl\n",
    "assert np.any(dtl.tree == dtl2.tree)\n",
    "\n",
    "dtl2.get_learner_info()\n",
    "\n",
    "# Modify the dtl2.tree and assert that this doesn't affect dtl.tree\n",
    "dtl2.tree[0] = np.arange(dtl2.tree.shape[1])\n",
    "assert np.any(dtl.tree != dtl2.tree)\n",
    "\n",
    "# Query with dummy data\n",
    "dtl.query(np.array([[1, 2, 3], [0.2, 12, 12]]))\n",
    "\n",
    "# Another dataset to test that \"If the best feature doesn't split the data into two\n",
    "# groups, choose the second best one and so on; if none of the features does, return leaf\"\n",
    "x2 = np.array([\n",
    " [  0.26,    0.63,   11.8  ],\n",
    " [  0.26,    0.63,   11.8  ],\n",
    " [  0.32,    0.78,   10.   ],\n",
    " [  0.32,    0.78,   10.   ],\n",
    " [  0.32,    0.78,   10.   ],\n",
    " [  0.735,   0.57,    9.8  ],\n",
    " [  0.26,    0.63,   11.8  ],\n",
    " [  0.61,    0.63,    8.4  ]])\n",
    "\n",
    "y2 = np.array([ 8.,  8.,  6.,  6.,  6.,  5.,  8.,  3.])\n",
    "\n",
    "dtl = DTLearner(verbose=True)\n",
    "dtl.addEvidence(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegLearner(object):  \n",
    "    \n",
    "    def __init__(self, verbose = False):\n",
    "        pass\n",
    "    \n",
    "    def addEvidence(self,dataX,dataY):  \n",
    "        \"\"\" \n",
    "        @summary: Add training data to learner  \n",
    "        @param dataX: X values of data to add  \n",
    "        @param dataY: the Y training values  \n",
    "        \"\"\"  \n",
    "        # slap on 1s column so linear regression finds a constant term \n",
    "        newdataX = np.ones([dataX.shape[0],dataX.shape[1]+1])  \n",
    "        newdataX[:,0:dataX.shape[1]]=dataX  \n",
    " \n",
    "        # build and save the model  \n",
    "        self.model_coefs, residuals, rank, s = np.linalg.lstsq(newdataX, dataY)  \n",
    " \n",
    "    def query(self,points): \n",
    "        \"\"\"  \n",
    "        @summary: Estimate a set of test points given the model we built.  \n",
    "        @param points: should be a numpy array with each row corresponding to a specific query.  \n",
    "        @returns the estimated values according to the saved model.  \n",
    "        \"\"\"  \n",
    "        return (self.model_coefs[:-1] * points).sum(axis = 1) + self.model_coefs[-1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare two learners' rmse out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_os_rmse(learner1, learner2, X, Y):  \n",
    "    # compute how much of the data is training and testing   \n",
    "    train_rows = int(math.floor(0.6* X.shape[0])) \n",
    "    test_rows = X.shape[0] - train_rows     \n",
    " \n",
    "    # separate out training and testing data  \n",
    "    train = np.random.choice(X.shape[0], size=train_rows, replace=False) \n",
    "    test = np.setdiff1d(np.array(range(X.shape[0])), train)  \n",
    "    trainX = X[train, :]\n",
    "    trainY = Y[train]  \n",
    "    testX = X[test, :]  \n",
    "    testY = Y[test] \n",
    "    \n",
    "    # train the learners \n",
    "    learner1.addEvidence(trainX, trainY) # train it  \n",
    "    learner2.addEvidence(trainX, trainY) # train it \n",
    "\n",
    "    # evaluate learner1 out of sample  \n",
    "    predY = learner1.query(testX) # get the predictions \n",
    "    rmse1 = math.sqrt(((testY - predY) ** 2).sum()/testY.shape[0])    \n",
    "\n",
    "    # evaluate learner2 out of sample \n",
    "    predY = learner2.query(testX) # get the predictions \n",
    "    rmse2 = math.sqrt(((testY - predY) ** 2).sum()/testY.shape[0])\n",
    "    return rmse1, rmse2 \n",
    "\n",
    "def test_code(): \n",
    "    # create two learners and get data  \n",
    "    lrlearner = LinRegLearner(verbose = False)   \n",
    "    dtlearner = DTLearner(verbose = False, leaf_size = 1)   \n",
    "    X, Y = best4LinReg()  \n",
    "\n",
    "    # compare the two learners  \n",
    "    rmseLR, rmseDT = compare_os_rmse(lrlearner, dtlearner, X, Y)  \n",
    "    print(\"rmseLR, rmseDT:\", rmseLR, rmseDT)\n",
    "    # share results    \n",
    "    print \n",
    "    print(\"best4LinReg() results\")\n",
    "    print(\"RMSE LR    : \", rmseLR)  \n",
    "    print(\"RMSE DT    : \", rmseDT)\n",
    "    if rmseLR < 0.9 * rmseDT:  \n",
    "        print(\"LR < 0.9 DT:  pass\")\n",
    "    else:  \n",
    "        print(\"LR >= 0.9 DT:  fail\")  \n",
    "    print  \n",
    "  \n",
    "    # get data that is best for a random tree  \n",
    "    lrlearner = LinRegLearner(verbose = False) \n",
    "    dtlearner = DTLearner(verbose = False, leaf_size = 1) \n",
    "    X, Y = best4DT()  \n",
    "\n",
    "    # compare the two learners  \n",
    "    rmseLR, rmseDT = compare_os_rmse(lrlearner, dtlearner, X, Y)  \n",
    "\n",
    "    # share results  \n",
    "    print  \n",
    "    print(\"best4RT() results\")\n",
    "    print(\"RMSE LR    : \", rmseLR)  \n",
    "    print(\"RMSE RT    : \", rmseDT)  \n",
    "    if rmseDT < 0.9 * rmseLR:  \n",
    "        print(\"DT < 0.9 LR:  pass\")  \n",
    "    else:    \n",
    "        print(\"DT >= 0.9 LR:  fail\") \n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/home/emi/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:3013: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  prob = _betai(0.5*df, 0.5, df/(df+t_squared))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmseLR, rmseDT: 1.4681229241792225e-15 0.6379561784404958\n",
      "best4LinReg() results\n",
      "RMSE LR    :  1.4681229241792225e-15\n",
      "RMSE DT    :  0.6379561784404958\n",
      "LR < 0.9 DT:  pass\n",
      "best4RT() results\n",
      "RMSE LR    :  4.1262487946016675\n",
      "RMSE RT    :  5.017815451510112\n",
      "DT >= 0.9 LR:  fail\n"
     ]
    }
   ],
   "source": [
    "test_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
